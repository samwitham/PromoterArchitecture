{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleles_frequency_table.zip can be unzipped to a tab-separated text file that shows all reads and alignments to references. The first column shows the aligned sequence of the sequenced read. The second column shows the aligned sequence of the reference sequence. Gaps in each of these columns represent insertions and deletions. The next column 'Reference_Name' shows the name of the reference that the read aligned to. The fourth column, 'Read_Status' shows whether the read was modified or unmodified. The fifth through seventh columns ('n_deleted', 'n_inserted', 'n_substituted') show the number of bases deleted, inserted, and substituted as compared to the reference sequence. The eighth column shows the number of reads having that sequence, and the ninth column shows the percentage of all reads having that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #features to add:\n",
    "# Distance from TSS - get relative position of mutation in the guide site - done. Add distance from cut site metric. - done Then calculate distance from Araport TSS - done\n",
    "# for this: first create a bed file for all of the mutations (relative to whole Arabidopsis genome). Then do bedtools merge or intersect (or bedtools coverage (../data_sorting/./TFBS_coverage.sh)) with the mapped motif bed file (all TFBSs for all genes). Record each TFBS that overlaps the mutation\n",
    "# Overlapping TFBSs - subnet\n",
    "# work and all TFs\n",
    "\n",
    "# Include secondary mutations in case both deletion and substitution for example - done\n",
    "# Plant ID\n",
    "# How many biallelic or homozygous? How many wildtype?\n",
    "# More than 2 alleles for a gene - record alleles until 80% of reads accounted for\n",
    "# Prioritise homozygous or biallelic\n",
    "# How many plants had mutations? How many guides produced mutations in each gene?\n",
    "#check window around cut site - at the moment I am including mutations 20bp either side, maybe cut the alignments down to 7bp either side before comparing them with find_indels_substitutions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use env pybedtools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pybedtools import BedTool\n",
    "from collections import defaultdict\n",
    "#chunks\n",
    "from more_itertools import sliced\n",
    "from itertools import combinations\n",
    "#import multiprocessing as mp\n",
    "\n",
    "#import time\n",
    "#import os\n",
    "import math\n",
    "#import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_files(mutations_file,mapped_motifs,guide_pairs):\n",
    "    \"\"\"read in the files\"\"\"\n",
    "    #read in mapped motifs bed file\n",
    "    mapped_motifs = pd.read_table(mapped_motifs_bed, sep=\"\\t\", header=None)\n",
    "    if len(mapped_motifs.columns) == 24:\n",
    "        cols = [\n",
    "            \"chr\",\n",
    "            \"start\",\n",
    "            \"stop\",\n",
    "            \"promoter_AGI\",\n",
    "            \"dot1\",\n",
    "            \"strand\",\n",
    "            \"source\",\n",
    "            \"type\",\n",
    "            \"dot2\",\n",
    "            \"attributes\",\n",
    "            \"motif_chr\",\n",
    "            \"motif_start\",\n",
    "            \"motif_stop\",\n",
    "            \"name_rep\",\n",
    "            \"score\",\n",
    "            \"motif_strand\",\n",
    "            \"promoter_AGI2\",\n",
    "            \"p-value\",\n",
    "            \"q-value\",\n",
    "            \"matched_sequence\",\n",
    "            \"TF_name\",\n",
    "            \"TF_family\",\n",
    "            \"TF_AGI\",\n",
    "            \"bp_overlap\",\n",
    "        ]\n",
    "        mapped_motifs.columns = cols\n",
    "        # filter columns\n",
    "        mapped_motifs = mapped_motifs[\n",
    "            [\n",
    "                \"motif_chr\",\n",
    "                \"motif_start\",\n",
    "                \"motif_stop\",\n",
    "                \"name_rep\",\n",
    "                \"score\",\n",
    "                \"motif_strand\",\n",
    "                \"promoter_AGI2\",\n",
    "                \"p-value\",\n",
    "                \"q-value\",\n",
    "                \"matched_sequence\",\n",
    "                \"TF_name\",\n",
    "                \"TF_family\",\n",
    "                \"TF_AGI\",\n",
    "            ]\n",
    "        ]\n",
    "        #rename columns\n",
    "        cols = [\n",
    "            \"motif_chr\",\n",
    "            \"motif_start\",\n",
    "            \"motif_stop\",\n",
    "            \"name_rep\",\n",
    "            \"score\",\n",
    "            \"motif_strand\",\n",
    "            \"promoter_AGI\",\n",
    "            \"p-value\",\n",
    "            \"q-value\",\n",
    "            \"matched_sequence\",\n",
    "            \"TF_name\",\n",
    "            \"TF_family\",\n",
    "            \"TF_AGI\",\n",
    "        ]\n",
    "        mapped_motifs.columns = cols\n",
    "\n",
    "    elif len(mapped_motifs.columns) == 13:\n",
    "        cols = [\n",
    "            \"motif_chr\",\n",
    "            \"motif_start\",\n",
    "            \"motif_stop\",\n",
    "            \"name_rep\",\n",
    "            \"score\",\n",
    "            \"motif_strand\",\n",
    "            \"promoter_AGI\",\n",
    "            \"p-value\",\n",
    "            \"q-value\",\n",
    "            \"matched_sequence\",\n",
    "            \"TF_name\",\n",
    "            \"TF_family\",\n",
    "            \"TF_AGI\",\n",
    "        ]\n",
    "        mapped_motifs.columns = cols\n",
    "\n",
    "    elif len(mapped_motifs.columns) == 17:\n",
    "        cols = [\n",
    "            \"motif_chr\",\n",
    "            \"motif_start\",\n",
    "            \"motif_stop\",\n",
    "            \"name_rep\",\n",
    "            \"score\",\n",
    "            \"motif_strand\",\n",
    "            \"promoter_AGI\",\n",
    "            \"p-value\",\n",
    "            \"q-value\",\n",
    "            \"matched_sequence\",\n",
    "            \"TF_name\",\n",
    "            \"TF_family\",\n",
    "            \"TF_AGI\",\n",
    "            \"chr_openchrom\",\n",
    "            \"start_openchrom\",\n",
    "            \"stop_openchrom\",\n",
    "            \"bp_overlap\",\n",
    "        ]\n",
    "        mapped_motifs.columns = cols\n",
    "\n",
    "    mutations_df = pd.read_table(mutations_file,sep='\\t',header=0)\n",
    "    guide_pairs_df = pd.read_csv(guide_pairs,header=0)\n",
    "    #only keep first 2 columns\n",
    "    guide_cols = ['guide1','guide2']\n",
    "    guide_pairs_df = guide_pairs_df[guide_cols]\n",
    "    return mutations_df,mapped_motifs,guide_pairs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bedfiles(bedfile, mapped_motifs_bed, output_buffer):\n",
    "    \"\"\"perform bedtools intersect on the two dfs\"\"\"\n",
    "    df = BedTool(bedfile)\n",
    "    motifs = BedTool(mapped_motifs_bed)\n",
    "    # -wao =Write the original A and B entries plus the number of base pairs of overlap between the two features.\n",
    "    # However, A features w/o overlap are also reported with a NULL B feature and overlap = 0\n",
    "    intersect = df.intersect(motifs, wao=True)\n",
    "    # Write to output_file\n",
    "    # Each line in the file contains bed entry a and bed entry b that it overlaps plus the number of bp in the overlap so 19 columns\n",
    "    output_buffer.write(str(intersect))\n",
    "    #go back to beginning of buffer\n",
    "    output_buffer.seek(0)\n",
    "    mapped_motifs_bed.seek(0)\n",
    "    return output_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlapping_TFBSs(mutations_df_chunk,mapped_motifs_bed,mapped_motifs_bed_columns):\n",
    "    \"\"\"function to find any overlapping TFBSs from FIMO mapped motif file\"\"\"\n",
    "    #go back to start of buffer\n",
    "    mapped_motifs_bed.seek(0)\n",
    "\n",
    "    #turn mutations_df_chunk into list of dicts\n",
    "    mutations_df_chunk_dict = mutations_df_chunk.to_dict(orient=\"records\")\n",
    "\n",
    "    for mutations_df_row_dict in mutations_df_chunk_dict:\n",
    "        #mutations_df_index = mutations_df_row.index\n",
    "            \n",
    "        if mutations_df_row_dict['mutation_type'] == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            #create temporary df in bed format\n",
    "            \n",
    "            cols = [\"chr\",\n",
    "            \"start\",\n",
    "            \"stop\",\n",
    "            \"mutation_type\",'mutation_count']\n",
    "            temp_df = pd.DataFrame(columns=cols)\n",
    "            chr = mutations_df_row_dict['chr']\n",
    "            #if not NaN\n",
    "            if pd.notna(mutations_df_row_dict['insertion_genomic_positions']) and mutations_df_row_dict['insertion_genomic_positions'] != 'nan':\n",
    "                #print(mutations_df_row)\n",
    "                #print(\"Index:\", index)\n",
    "                #print(row.insertion_genomic_positions)\n",
    "                #convert genomic positions from string to list\n",
    "                # Convert string to list if more than one\n",
    "                insertion_genomic_positions = mutations_df_row_dict['insertion_genomic_positions'].strip('][').split(', ')\n",
    "                #count which mutation number currently on to be added to the temporary bed file\n",
    "                count = 0\n",
    "                for gen_pos in insertion_genomic_positions:\n",
    "                    count += 1\n",
    "                    #get index\n",
    "                    # if len(insertion_genomic_positions) > 1:\n",
    "                    #     index = insertion_genomic_positions.index(gen_pos)\n",
    "                    # else:\n",
    "                    #     index = 'nan'\n",
    "                    start = int(gen_pos)\n",
    "                    stop = start + 1\n",
    "                    mutation_type = \"insertion\"\n",
    "                    #add to temp_df\n",
    "                    temp_list = [chr,start,stop,mutation_type,count]\n",
    "                    temp_df.loc[len(temp_df)] = temp_list\n",
    "            if pd.notna(mutations_df_row_dict['deletion_genomic_positions']) and mutations_df_row_dict['deletion_genomic_positions'] != 'nan':\n",
    "                # Convert string to list\n",
    "                deletion_genomic_positions = mutations_df_row_dict['deletion_genomic_positions'].strip('][').split(', ')\n",
    "                #count which mutation number currently on to be added to the temporary bed file\n",
    "                count = 0\n",
    "                for gen_pos in deletion_genomic_positions:\n",
    "                    count += 1\n",
    "                    #get index\n",
    "                    # if len(deletion_genomic_positions) > 1:\n",
    "                    #     index = deletion_genomic_positions.index(gen_pos)\n",
    "                    # else:\n",
    "                    #     index = 'nan'\n",
    "                    start = int(gen_pos)\n",
    "                    stop = start + 1\n",
    "                    mutation_type = \"deletion\"\n",
    "                    #add to temp_df\n",
    "                    temp_list = [chr,start,stop,mutation_type,count]\n",
    "                    temp_df.loc[len(temp_df)] = temp_list\n",
    "            if pd.notna(mutations_df_row_dict['substitution_genomic_positions']) and mutations_df_row_dict['substitution_genomic_positions'] != 'nan':\n",
    "                # Convert string to list\n",
    "                substitution_genomic_positions = mutations_df_row_dict['substitution_genomic_positions'].strip('][').split(', ')\n",
    "                #count which mutation number currently on to be added to the temporary bed file\n",
    "                count = 0\n",
    "                for gen_pos in substitution_genomic_positions:\n",
    "                    count += 1\n",
    "                    #get index\n",
    "                    # if len(substitution_genomic_positions) > 1:\n",
    "                    #     index = substitution_genomic_positions.index(gen_pos)\n",
    "                    # else:\n",
    "                    #     index = 'nan'\n",
    "                    start = int(gen_pos)\n",
    "                    stop = start + 1\n",
    "                    mutation_type = \"substitution\"\n",
    "                    #add to temp_df\n",
    "                    temp_list = [chr,start,stop,mutation_type,count]\n",
    "                    temp_df.loc[len(temp_df)] = temp_list\n",
    "            #now do bedtools intersect to find which TFBSs overlap with which mutations\n",
    "            #sort by chr then start\n",
    "            temp_df = temp_df.sort_values([\"chr\", \"start\"]).reset_index(drop=True)\n",
    "            # write to buffer\n",
    "            temp_df_buffer = io.StringIO()\n",
    "            temp_df.to_csv(temp_df_buffer, sep=\"\\t\", index=False, header=None)\n",
    "            temp_df_buffer.seek(0)\n",
    "            \n",
    "            output_buffer = io.StringIO()\n",
    "        \n",
    "            output_buffer = merge_bedfiles(temp_df_buffer, mapped_motifs_bed, output_buffer)\n",
    "            \n",
    "            #remove temp_df_buffer stream\n",
    "\n",
    "\n",
    "            #read in output buffer as df\n",
    "            output_df = pd.read_table(output_buffer, sep='\\t')\n",
    "            \n",
    "            #get column names and rename columns\n",
    "            output_df_cols = cols+mapped_motifs_bed_columns+['bp_overlap']\n",
    "            output_df.columns = output_df_cols\n",
    "            #for each mutation type get list of overlapping TFBSs. Add these to a dictionary column in the mutations_df_chunk\n",
    "            #create defaultdicts with lists as values so that non-existing keys can be added to in one go\n",
    "            insertion_overlapping_TFBS_family = defaultdict(list)\n",
    "            insertion_overlapping_TFBS_AGI = defaultdict(list)\n",
    "            #insertion_overlapping_TFBS_total = defaultdict(list)\n",
    "            deletion_overlapping_TFBS_family = defaultdict(list)\n",
    "            deletion_overlapping_TFBS_AGI = defaultdict(list)\n",
    "            #deletion_overlapping_TFBS_total = defaultdict(list)\n",
    "            substitution_overlapping_TFBS_family = defaultdict(list)\n",
    "            substitution_overlapping_TFBS_AGI = defaultdict(list)\n",
    "            #substitution_overlapping_TFBS_total = defaultdict(list)\n",
    "            \n",
    "            #convert output_df into dict\n",
    "            output_df_dict = output_df.to_dict(orient=\"records\")\n",
    "            for row_dict in output_df_dict:\n",
    "                # empty number that will increase for each insertion\n",
    "               # insertion_overlapping_TFBS_count = int()\n",
    "               #if mutation and has a 1bp overlap\n",
    "               #if no TF_AGI then pass\n",
    "                if row_dict['TF_AGI'] == '.':\n",
    "                    pass\n",
    "                elif row_dict['mutation_type'] == 'insertion' and row_dict['bp_overlap'] > 0:\n",
    "                    #add insertion TFBS family information to dictionary for the correct mutation number\n",
    "                    #print(row)\n",
    "                    mut_count = row_dict['mutation_count']\n",
    "                    insertion_overlapping_TFBS_family[f'insertion{mut_count}'] += [row_dict['TF_family']]\n",
    "                    #add insertion TFBS AGI information to dictionary for the correct mutation number\n",
    "                    insertion_overlapping_TFBS_AGI[f'insertion{mut_count}'] += [row_dict['TF_AGI']]\n",
    "                    #add total number of TFBSs overlapping each insertion\n",
    "                 #   insertion_overlapping_TFBS_total[f'insertion{row.mutation_count}'] += 1\n",
    "                elif row_dict['mutation_type'] == 'deletion' and row_dict['bp_overlap'] > 0:\n",
    "                    #add deletion TFBS family information to dictionary for the correct mutation number\n",
    "                    mut_count = row_dict['mutation_count']\n",
    "                    deletion_overlapping_TFBS_family[f'deletion{mut_count}'] += [row_dict['TF_family']]\n",
    "                    #add deletion TFBS AGI information to dictionary for the correct mutation number\n",
    "                    deletion_overlapping_TFBS_AGI[f'deletion{mut_count}'] += [row_dict['TF_AGI']]\n",
    "                    #add total number of TFBSs overlapping each insertion\n",
    "                   # deletion_overlapping_TFBS_total[f'deletion{row.mutation_count}'] += 1\n",
    "                elif row_dict['mutation_type'] == 'substitution' and row_dict['bp_overlap'] > 0:\n",
    "                    #add substitution TFBS family information to dictionary for the correct mutation number\n",
    "                    mut_count = row_dict['mutation_count']\n",
    "                    substitution_overlapping_TFBS_family[f'substitution{mut_count}'] += [row_dict['TF_family']]\n",
    "                    #add substitution TFBS AGI information to dictionary for the correct mutation number\n",
    "                    substitution_overlapping_TFBS_AGI[f'substitution{mut_count}'] += [row_dict['TF_AGI']]\n",
    "                    #add total number of TFBSs overlapping each insertion\n",
    "                 #   substitution_overlapping_TFBS_total[f'substitution{row.mutation_count}'] += 1\n",
    "            # #calculate total unique TFBS for each insertion, deletion and subsitution\n",
    "            # insertion_overlapping_TFBS_total_unique = []\n",
    "            # for insertion,AGI in insertion_overlapping_TFBS_AGI.items():\n",
    "            #     insertion_overlapping_TFBS_total_unique += np.unique(AGI).astype(list)\n",
    "            # deletion_overlapping_TFBS_total_unique = []\n",
    "            # for deletion,AGI in deletion_overlapping_TFBS_AGI.items():\n",
    "            #     print(f'AGIunique={np.unique(AGI).astype(list)}')\n",
    "            #     print(f'total={deletion_overlapping_TFBS_total_unique}')\n",
    "            #     print(f'AGI={AGI}')\n",
    "            #     deletion_overlapping_TFBS_total_unique += np.unique(AGI).astype(list)\n",
    "            #     print(f'newtotal={deletion_overlapping_TFBS_total_unique}')\n",
    "            # substitution_overlapping_TFBS_total_unique = []\n",
    "            # for substitution,AGI in substitution_overlapping_TFBS_AGI.items():\n",
    "            #     substitution_overlapping_TFBS_total_unique += np.unique(AGI).astype(list)\n",
    "                \n",
    "            #add values to mutations_df row\n",
    "            #print(f'insertion_overlapping_TFBS_family={dict(insertion_overlapping_TFBS_family)}')\n",
    "            #print(mutations_df_row)\n",
    "            #first make overlapping TFBS families and AGIs unique\n",
    "            insertion_overlapping_TFBS_family = dict(insertion_overlapping_TFBS_family)\n",
    "            insertion_overlapping_TFBS_AGI = dict(insertion_overlapping_TFBS_AGI)\n",
    "            deletion_overlapping_TFBS_family = dict(deletion_overlapping_TFBS_family)\n",
    "            deletion_overlapping_TFBS_AGI = dict(deletion_overlapping_TFBS_AGI)\n",
    "            substitution_overlapping_TFBS_family = dict(substitution_overlapping_TFBS_family)\n",
    "            substitution_overlapping_TFBS_AGI = dict(substitution_overlapping_TFBS_AGI)\n",
    "            #if empty dictionary, change to nan\n",
    "            if insertion_overlapping_TFBS_family == {}:\n",
    "                insertion_overlapping_TFBS_family = np.nan\n",
    "            else:\n",
    "                #keep only unique TFBS families\n",
    "                for k,v in insertion_overlapping_TFBS_family.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    insertion_overlapping_TFBS_family[k] = np.unique(v).tolist()\n",
    "\n",
    "            if insertion_overlapping_TFBS_AGI == {}:\n",
    "                insertion_overlapping_TFBS_AGI = np.nan\n",
    "            else:\n",
    "                #keep only unique TFBS AGIs\n",
    "                for k,v in insertion_overlapping_TFBS_AGI.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    insertion_overlapping_TFBS_AGI[k] = np.unique(v).tolist()\n",
    "\n",
    "            if deletion_overlapping_TFBS_family == {}:\n",
    "                deletion_overlapping_TFBS_family = 'nan'\n",
    "            else:\n",
    "                #keep only unique TFBS families\n",
    "                for k,v in deletion_overlapping_TFBS_family.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    deletion_overlapping_TFBS_family[k] = np.unique(v).tolist()\n",
    "\n",
    "            if deletion_overlapping_TFBS_AGI == {}:\n",
    "                deletion_overlapping_TFBS_AGI = np.nan\n",
    "            else:\n",
    "                #keep only unique TFBS AGIs\n",
    "                for k,v in deletion_overlapping_TFBS_AGI.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    deletion_overlapping_TFBS_AGI[k] = np.unique(v).tolist()\n",
    "            \n",
    "            if substitution_overlapping_TFBS_family == {}:\n",
    "                substitution_overlapping_TFBS_family = np.nan\n",
    "            else:\n",
    "                #keep only unique TFBS families\n",
    "                for k,v in substitution_overlapping_TFBS_family.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    substitution_overlapping_TFBS_family[k] = np.unique(v).tolist()\n",
    "                    \n",
    "            if substitution_overlapping_TFBS_AGI == {}:\n",
    "                substitution_overlapping_TFBS_AGI = np.nan\n",
    "            else:\n",
    "                #keep only unique TFBS AGIs\n",
    "                for k,v in substitution_overlapping_TFBS_AGI.items():\n",
    "                    #print(np.unique(v).astype(list))\n",
    "                    substitution_overlapping_TFBS_AGI[k] = np.unique(v).tolist()\n",
    "            #append values to mutations_df_row_dict\n",
    "            mutations_df_row_dict['insertion_overlapping_TFBS_family']=str(insertion_overlapping_TFBS_family)\n",
    "            mutations_df_row_dict['insertion_overlapping_TFBS_AGI']=str(insertion_overlapping_TFBS_AGI)\n",
    "            mutations_df_row_dict['deletion_overlapping_TFBS_family']=str(deletion_overlapping_TFBS_family)\n",
    "            mutations_df_row_dict['deletion_overlapping_TFBS_AGI']=str(deletion_overlapping_TFBS_AGI)\n",
    "            mutations_df_row_dict['substitution_overlapping_TFBS_family']=str(substitution_overlapping_TFBS_family)\n",
    "            mutations_df_row_dict['substitution_overlapping_TFBS_AGI']=str(substitution_overlapping_TFBS_AGI)\n",
    "\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"insertion_overlapping_TFBS_family\"] = str(insertion_overlapping_TFBS_family)\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"insertion_overlapping_TFBS_AGI\"]= str(insertion_overlapping_TFBS_AGI)\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"deletion_overlapping_TFBS_family\"]= str(deletion_overlapping_TFBS_family)\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"deletion_overlapping_TFBS_AGI\"]= str(deletion_overlapping_TFBS_AGI)\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"substitution_overlapping_TFBS_family\"]= str(substitution_overlapping_TFBS_family)\n",
    "            # mutations_df_chunk.loc[mutations_df_index, \"substitution_overlapping_TFBS_AGI\"]= str(substitution_overlapping_TFBS_AGI)\n",
    "            # row.insertion_overlapping_TFBS_total_unique= insertion_overlapping_TFBS_total_unique\n",
    "            # row.deletion_overlapping_TFBS_total_unique= deletion_overlapping_TFBS_total_unique\n",
    "            # row.substitution_overlapping_TFBS_total_unique= substitution_overlapping_TFBS_total_unique\n",
    "            #mutations_df_row = \n",
    "            #go back to start of buffer\n",
    "            mapped_motifs_bed.seek(0)\n",
    "    #return mutations_df_row_dict as a df\n",
    "    return pd.DataFrame.from_dict(mutations_df_chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(mutations_df,mapped_motifs_df,output_folder,gene):\n",
    "    \"\"\"function to prepare dfs and slice mutations_df into chunks to reduce memory before running find_overlapping_TFBSs() \"\"\"\n",
    "   \n",
    "\n",
    "    #for each guide containing mutations, create a temporary bed file containing each mutation and then do bedtools intersect to find which overlap TFBs\n",
    "    #then add the TFBS names into a new column for that row\n",
    "\n",
    "    #get column names from mapped_motifs_df\n",
    "    mapped_motifs_bed_columns = list(mapped_motifs_df.columns)\n",
    "\n",
    "\n",
    "    #turn mapped_motifs_df into a buffer\n",
    "    mapped_motifs_bed = io.StringIO()\n",
    "    mapped_motifs_df.to_csv(mapped_motifs_bed, sep=\"\\t\", index=False, header=None)\n",
    "    #go back to start of buffer\n",
    "    mapped_motifs_bed.seek(0)\n",
    "\n",
    "    #add columns to mutations_df\n",
    "    new_columns = ['insertion_overlapping_TFBS_family',\n",
    "    'insertion_overlapping_TFBS_AGI',\n",
    "    'deletion_overlapping_TFBS_family',\n",
    "    'deletion_overlapping_TFBS_AGI',\n",
    "    'substitution_overlapping_TFBS_family',\n",
    "    'substitution_overlapping_TFBS_AGI',\n",
    "                  ]\n",
    "    #first make a new df that will merge into mutations_df\n",
    "    temp_new_df = pd.DataFrame(columns=new_columns)\n",
    "    mutations_df = pd.concat([mutations_df,temp_new_df], axis=1)\n",
    "    #print(mutations_df)\n",
    "\n",
    "    #first make certain columns string\n",
    "    #make columns containing lists string for now so can use groupby\n",
    "    to_string = ['insertion_positions','deletion_positions','substitution_positions','insertion_cut_site_distance','deletion_cut_site_distance','substitution_cut_site_distance','insertion_positions_relative_to_TSS','insertion_genomic_positions','deletion_positions_relative_to_TSS','deletion_genomic_positions','substitution_positions_relative_to_TSS','substitution_genomic_positions']\n",
    "    mutations_df[to_string] = mutations_df[to_string].astype(str)\n",
    "\n",
    "\n",
    "    # chunksize = 5\n",
    "    # def df_chunking(df,mapped_motifs_bed,mapped_motifs_bed_columns, chunksize):\n",
    "    #     \"\"\"Splits df into chunks, drops data of original df inplace\"\"\"\n",
    "    #     count = 0 # Counter for chunks\n",
    "    #     #create list of chunks\n",
    "    #     chunks = []\n",
    "    #     while len(df):\n",
    "    #         count += 1\n",
    "    #         print('Preparing chunk {}'.format(count))\n",
    "    #         # Return df chunk\n",
    "    #         chunk = df.iloc[:chunksize].copy()\n",
    "    #         #go back to start of buffer\n",
    "    #         mapped_motifs_bed.seek(0)\n",
    "    #         new_chunk = find_overlapping_TFBSs(chunk,mapped_motifs_bed,mapped_motifs_bed_columns)\n",
    "    #         #append chunk to list\n",
    "    #         chunks.append(new_chunk)\n",
    "    #         # Delete data in place because it is no longer needed\n",
    "    #         df.drop(df.index[:chunksize], inplace=True)\n",
    "    #     return chunks\n",
    "\n",
    "    # chunks = df_chunking(mutations_df,mapped_motifs_bed,mapped_motifs_bed_columns, chunksize)\n",
    "  \n",
    "#     # Job parameters\n",
    "#     n_jobs = 4  # Poolsize\n",
    "#     chunksize = 5  # Maximum size of Frame Chunk\n",
    "#     # Preparation\n",
    "#    # df = pd.DataFrame(np.random.rand(*size))\n",
    "#     ctx = mp.get_context('spawn')\n",
    "#     pool = ctx.Pool(n_jobs)\n",
    "#     #pool = mp.Pool(n_jobs)\n",
    "#     print('Starting MP')\n",
    "#     # Execute the wait and print function in parallel\n",
    "#     #create list of chunks\n",
    "#     # chunks = []\n",
    "#     #mutations_df = pd.concat(pool.imap(find_overlapping_TFBSs(df,mapped_motifs_bed,mapped_motifs_bed_columns), df_chunking(df, chunksize)))\n",
    "#     # mutations_df = pd.concat(pool.starmap(lambda mutations_df: find_overlapping_TFBSs(mutations_df,mapped_motifs_bed,mapped_motifs_bed_columns), df_chunking(mutations_df, chunksize)))\n",
    "#     #mutations_df = pd.concat(pool.starmap(find_overlapping_TFBSs,[(mutations_df,mapped_motifs_bed,mapped_motifs_bed_columns)], df_chunking(mutations_df, chunksize)))\n",
    "#     mutations_df = pd.concat(pool.starmap(df_chunking,[(mutations_df, chunksize)]))\n",
    "#     # p.starmap(print_name, [('Thomas', 'Scott'), ('Ali', 'Khan')])\n",
    "#     #concatenate the chunks\n",
    "#    # mutations_df = pd.concat(p.map(Simulation, range(10))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "#     print('DONE')\n",
    "\n",
    "    #convert mutations_df into chunks to reduce memory load\n",
    "    CHUNK_SIZE = 5\n",
    "\n",
    "    index_slices = sliced(range(len(mutations_df)), CHUNK_SIZE)\n",
    "    #create list of chunks\n",
    "    chunks = []\n",
    "    for index_slice in index_slices:\n",
    "        #go back to start of buffer\n",
    "        mapped_motifs_bed.seek(0)\n",
    "        chunk = mutations_df.iloc[index_slice] # your dataframe chunk ready for use\n",
    "        new_chunk = find_overlapping_TFBSs(chunk,mapped_motifs_bed,mapped_motifs_bed_columns)\n",
    "\n",
    "        \n",
    "        chunks.append(new_chunk)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    #concatenate chunks into mutations_df\n",
    "    mutations_df = pd.concat(chunks)            \n",
    "    #write out mutations_df\n",
    "    mutations_df.to_csv(f'{output_folder}{gene}_TFBSoverlapping.tsv', sep=\"\\t\", index=False, header=1)\n",
    "                     \n",
    "    return mutations_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#written in dask format rather than pandas\n",
    "def genotype_plant_lines(mutations_df,output_folder,gene):\n",
    "    \"\"\"function to decide whether each plant line is -homozygous \n",
    "    -biallelic - each mutated at same location/site twice\n",
    "    -chimeric - different mutations in different cells or tissues - dont analyse if chimeric - 1 small region, if more than 2 mutations then probably chimeric - ie.    probably still has tDNA\n",
    "    Multiple guide sites - eg. multiple agro strains in each plant. Check for paired guides and whether guides that aren’t meant to be paired are paired.\n",
    "    \"\"\"\n",
    "    #homozygouslines are those with no duplicated guides in each plant line\n",
    "    mutations_df.loc[~mutations_df.duplicated(['plant_ID','guide']),'genotype'] = 'homozygous'\n",
    "    #mutations_df['genotype'] = mutations_df['genotype'].mask(~mutations_df.duplicated(['plant_ID','guide']), 'homozygous')\n",
    "    #biallelic\n",
    "    #N = 2\n",
    "    #mutations_df_copy = mutations_df[mutations_df.duplicated(['plant_ID','guide']) | mutations_df['count'].ge(N)]\n",
    "    #if 70% of reads or more are the same then mark as homozygous\n",
    "    mutations_df.loc[mutations_df.read_percentage >= 70, 'genotype'] = 'homozygous'\n",
    "    #if 10% of reads or less are that mutation, mark genotype as 'nan\n",
    "    mutations_df.loc[mutations_df.read_percentage <=10, 'genotype'] = 'nan'\n",
    "    #if between 10 and 70% of reads, mark genotype as heterozygous\n",
    "    mutations_df.loc[(mutations_df.read_percentage > 10) & (mutations_df.read_percentage < 70), 'genotype'] = 'heterozygous'\n",
    "\n",
    "    #create a count column\n",
    "    mutations_df['number_of_different_alleles'] = int()\n",
    "    #create non wild type count column\n",
    "    mutations_df['number_of_different_non_wt_alleles'] = int()\n",
    "    \n",
    "    #create a sum_of_count column\n",
    "    #mutations_df['sum_of_count'] = int()\n",
    "    #count how many duplicates there are for heterozygous lines.\n",
    "    mutations_df.loc[mutations_df.genotype == 'heterozygous','number_of_different_alleles'] = mutations_df[mutations_df.genotype == 'heterozygous'].groupby(['plant_ID','guide'])['number_of_different_alleles'].transform('count')\n",
    "    #make non wild type count\n",
    "    mutations_df.loc[(mutations_df.genotype == 'heterozygous')&~(mutations_df.mutation_type == 'None'),'number_of_different_non_wt_alleles'] = mutations_df[(mutations_df.genotype == 'heterozygous')&~(mutations_df.mutation_type == 'None')].groupby(['plant_ID','guide'])['number_of_different_non_wt_alleles'].transform('count')\n",
    "    #make count numeric\n",
    "    #mutations_df['count'] = mutations_df['count'].astype(int)\n",
    "    #print(mutations_df.dtypes)\n",
    "    #if heterozygous count is 1 then homozygous\n",
    "    mutations_df.loc[(mutations_df.genotype == 'heterozygous') & (mutations_df.number_of_different_alleles == 1),'genotype'] = 'homozygous'\n",
    "    #print(mutations_df.loc[(mutations_df.genotype == 'heterozygous') & (mutations_df.count == 1)])\n",
    "    #\n",
    "    #if homozygous then count is 1\n",
    "    mutations_df.loc[mutations_df.genotype == 'homozygous','number_of_different_alleles'] = 1\n",
    "\n",
    "    #if mutation_type is None, count is 0\n",
    "    #create mask for wildtype\n",
    "    # wt_mask = (mutations_df['mutation_type']=='None')\n",
    "    # mutations_df.loc[wt_mask, 'count'] = 0\n",
    "\n",
    "    #get the sum of the counts for each set of duplicates( wild type is 0, each mutation is counted as 1)\n",
    "    # sort=False, as_index=False\n",
    "\n",
    "   # mutations_df['sum_of_count'] = mutations_df.groupby(['plant_ID','guide'])['sum_of_count'].agg({\"count\":\"sum\"})\n",
    "    #print(mutations_df.groupby(['plant_ID','guide'],sort=False, as_index=False)['count'].agg({\"sum_of_count\":\"sum\"}))\n",
    "   # print(mutations_df.groupby(['plant_ID','guide'])['count'].transform('count'))\n",
    "\n",
    "\n",
    "\n",
    "    #if number_of_different_alleles is two and no wild type is present in either of the groups of reads then biallelic\n",
    "    mutations_df.loc[(mutations_df['number_of_different_non_wt_alleles']==2)&~(mutations_df.mutation_type == 'None'),'genotype'] = 'biallelic'\n",
    "    \n",
    "    #if number_of_different_alleles more than two then chimeric\n",
    "    mutations_df.loc[mutations_df['number_of_different_alleles']>2,'genotype'] = 'chimeric'\n",
    "\n",
    "    # reset count column\n",
    "    #filtered['count'] = filtered.groupby(['plant_ID','guide'])[['plant_ID','guide']].transform('count')\n",
    "\n",
    "    #df1.loc[df['count'] < N, 'count'] = 1\n",
    "    #mutations_df[mutations_df.duplicated('plant_ID','guide')]\n",
    "    #add plant\n",
    "    #print(mutations_df_copy)\n",
    "    #change column order\n",
    "    mutations_df = mutations_df[['chr', 'plant_ID', 'platename', 'library', 'first_reaction_primers',\n",
    "       'second_reaction_primers', 'guide', 'guide_number', 'aligned_sequence',\n",
    "       'reference_sequence', 'mutation_type','genotype', 'read_number', 'read_percentage',\n",
    "       'insertion_positions', 'deletion_positions', 'substitution_positions',\n",
    "       'insertion_cut_site_distance', 'deletion_cut_site_distance',\n",
    "       'substitution_cut_site_distance', 'cut_site_promoter_position',\n",
    "       'insertion_positions_relative_to_TSS', 'insertion_genomic_positions',\n",
    "       'deletion_positions_relative_to_TSS', 'deletion_genomic_positions',\n",
    "       'substitution_positions_relative_to_TSS',\n",
    "       'substitution_genomic_positions', 'insertion_overlapping_TFBS_family',\n",
    "       'insertion_overlapping_TFBS_AGI', 'deletion_overlapping_TFBS_family',\n",
    "       'deletion_overlapping_TFBS_AGI', 'substitution_overlapping_TFBS_family',\n",
    "       'substitution_overlapping_TFBS_AGI','number_of_different_alleles','number_of_different_non_wt_alleles'\n",
    "       ]]\n",
    "    #remove genotype 'nan'\n",
    "    mutations_df = mutations_df[~(mutations_df.genotype == 'nan')]\n",
    "    \n",
    "   \n",
    "   \n",
    "    #save df\n",
    "    mutations_df.to_csv(f'{output_folder}{gene}_TFBSoverlapping_genotyped.tsv', sep=\"\\t\", index=False, header=1)\n",
    "\n",
    "    return mutations_df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_guide_pairs(mutations_df_genotyped, guide_pairs_df, output_folder,gene):\n",
    "    \"\"\"function to check which guide pairs where delivered to each plant line and to check whether mutations at more than one guide site are within the other guide pair\"\"\"\n",
    "    #first get unique guides based on 2 columns in guide_pairs_df    \n",
    "    unique_guides = pd.concat([guide_pairs_df['guide1'],guide_pairs_df['guide2']]).unique()\n",
    "    #for guides in unique guides list, add dictionary value of all other potential guides that it is paired with\n",
    "    #create defaultdict with lists as values so that non-existing keys can be added to in one go\n",
    "    guide_dict = defaultdict(list)\n",
    "    for guide in unique_guides:\n",
    "        #check for instances of that guide in the first column\n",
    "        filtered_col_1 = guide_pairs_df[guide_pairs_df.guide1==guide]['guide2'].to_list()\n",
    "        #then do the same for the second column\n",
    "        filtered_col_2 = guide_pairs_df[guide_pairs_df.guide2==guide]['guide1'].to_list()\n",
    "        #create list of unique values\n",
    "        list_of_unique = np.unique((filtered_col_1 + filtered_col_2)).astype(list)\n",
    "        #append each value in list to dict\n",
    "        for val in list_of_unique:\n",
    "            guide_dict[guide] += [val]\n",
    "        \n",
    "        \n",
    "        \n",
    "    #turn into normal dict\n",
    "    guide_dict = dict(guide_dict)\n",
    "\n",
    "    #mutations_df_genotyped df, filter out non mutated guides. Then create a dictionary of guides which were mutated. Then compare the guide pairs in guide_dict to the mutated guides.\n",
    "    #make a new df with one plant line per row, detailing how many guides sites were mutated, which guide sites were mutated and then if more than 1 guide site is mutated\n",
    "    #finally, merge this df with the mutations_df_genotyped (applying the same value in each column within each plant line)\n",
    "    #filter out non mutated guides\n",
    "    only_mutated_guides = mutations_df_genotyped[~(mutations_df_genotyped.mutation_type=='None')]\n",
    "    #create a dictionary of guides which were mutated with plant ID as the key\n",
    "    #create dictionary of the only_mutated_guides df\n",
    "    only_mutated_guides_dict = only_mutated_guides.to_dict(orient=\"records\")\n",
    "    #create another dict with plant IDs as the key\n",
    "    plant_ID_guide_dict = defaultdict(list)\n",
    "    #iterate over only_mutated_guides_dict adding plant IDs as keys and mutated guide sites as values\n",
    "    for row_dict in only_mutated_guides_dict:\n",
    "        plant_ID = row_dict['plant_ID']       \n",
    "        guide = row_dict['guide']\n",
    "        #add to plant_ID_guide_dict if not there already\n",
    "        plant_ID_guide_dict[plant_ID] += [guide]\n",
    "        #remove duplicate guides from list\n",
    "        #current_list = plant_ID_guide_dict[plant_ID]\n",
    "        \n",
    "    #create a new df with only one row per plant ID and only containing plant IDs that aren't wild type\n",
    "    one_row_each_no_wildtype = only_mutated_guides.plant_ID.drop_duplicates(keep='first').to_frame()\n",
    "    #add guide pairs column\n",
    "    #one_row_each_no_wildtype['guide_pairs'] = ''\n",
    "   # one_row_each_no_wildtype_df = pd.DataFrame\n",
    "    #print(one_row_each_no_wildtype)\n",
    "    #print(plant_ID_guide_dict)\n",
    "    #print(guide_dict)\n",
    "    #for each plant line check whether the mutated guides fall within a guide pair\n",
    "    #firs turn into dict\n",
    "    one_row_each_no_wildtype_dict = one_row_each_no_wildtype.to_dict(orient=\"records\")\n",
    "    #print(one_row_each_no_wildtype_dict)\n",
    "    #turn into defaultdict\n",
    "    one_row_each_no_wildtype_ddict = []\n",
    "\n",
    "   # one_row_each_no_wildtype_ddict = defaultdict(dict,one_row_each_no_wildtype_dict)\n",
    "    #print(one_row_each_no_wildtype_ddict)\n",
    "    #iterate through dict add more values/columns names\n",
    "    for row_dict in one_row_each_no_wildtype_dict:\n",
    "        #create default dict of row\n",
    "        dd = defaultdict(list,row_dict)\n",
    "        #print(dd)\n",
    "        \n",
    "        plant_ID = row_dict['plant_ID']\n",
    "        plant_ID_guides = plant_ID_guide_dict[plant_ID]\n",
    "        dd['mutated_guide_sites'] += list(np.unique(plant_ID_guides))\n",
    "        #add list of guides to dict\n",
    "\n",
    "        #print(plant_ID_guides)\n",
    "        #add guide pair value\n",
    "        plant_ID_guide_pair_dict = defaultdict(list)\n",
    "        \n",
    "        for guide in dd['mutated_guide_sites']:\n",
    "            #create temporary list \n",
    "            temp_list = []\n",
    "            #get approved guide pairs\n",
    "            guide_pairs = guide_dict[guide]\n",
    "            #print(guide_pairs)\n",
    "            for g in guide_pairs:\n",
    "                temp_list.append((guide,g))\n",
    "            \n",
    "            #print(set(temp_list))\n",
    "            #print(temp_list)\n",
    "            #unique_list = list(np.unique(temp_list))\n",
    "            #append to approved guide pairs            \n",
    "            plant_ID_guide_pair_dict['guide_pairs'] += set(temp_list)\n",
    "\n",
    "        #check for approved guide pairs\n",
    "        guidepair_combos = list(combinations(dd['mutated_guide_sites'],2))\n",
    "        #print(plant_ID_guide_pair_dict['guide_pairs'])\n",
    "        guidepair_intersection =  list(set(guidepair_combos).intersection(plant_ID_guide_pair_dict['guide_pairs']))\n",
    "        non_approved_guidepairs = list(set(guidepair_combos).difference(plant_ID_guide_pair_dict['guide_pairs']))\n",
    "        #find unique guides in non_approved_guidepairs\n",
    "        unique_guidepair_intersection = list(np.unique(guidepair_intersection))\n",
    "        unique_nonapproved_guides = list(np.unique(non_approved_guidepairs))\n",
    "        #find guides in nonapproved guides list that aren't approved\n",
    "        list_of_nonapproved = list(set(unique_nonapproved_guides).difference(unique_guidepair_intersection))\n",
    "        #print(list_of_nonapproved)\n",
    "        #if guide in non_approved guidepairs is not in guidepair_intersection\n",
    "        #append to ddict\n",
    "        dd['approved_guide_pairs'] = guidepair_intersection\n",
    "        dd['guides_not_in_pairs'] = list_of_nonapproved\n",
    "        #print(guidepair_intersection)\n",
    "        #print(plant_ID_guide_pair_dict)\n",
    "        #estimate agro strain number\n",
    "        #round up the division using math.ceil\n",
    "        #first create list of mutated_guide_sites which aren't in approved pairs.\n",
    "        another_list = []\n",
    "        for pair in dd['approved_guide_pairs']:\n",
    "            another_list.append(pair)\n",
    "        another_unique_list = list(np.unique(another_list))\n",
    "        temp_set = list(set(dd['mutated_guide_sites']).difference(another_unique_list))\n",
    "        #divide mutated_guide_sites which aren't in approved pairs by 2, rounding up, and then add 1 for every approved guide pair\n",
    "        number_of_strains = int(math.ceil(len(temp_set)/2)) + len(dd['approved_guide_pairs'])\n",
    "        dd['estimated_agro_strain_number'] = number_of_strains\n",
    "\n",
    "        # if len(dd['approved_guide_pairs']) >1:\n",
    "        #     dd['agro_strain_number'] = \n",
    "\n",
    "        # # for guide_pair in combinations(plant_ID_guides,2):\n",
    "        #     print(guide_pair)\n",
    "\n",
    "        #approved_guide_pairs\n",
    "        #append defaultdict to list\n",
    "        one_row_each_no_wildtype_ddict.append(dd)\n",
    "    #turn ddict into df\n",
    "    df_single_rows = pd.DataFrame.from_dict(one_row_each_no_wildtype_ddict)\n",
    "    #merge this df with the only_mutated_guides df\n",
    "    only_mutated_guides = pd.merge(only_mutated_guides,df_single_rows, on='plant_ID', how='left')\n",
    "    #save this df\n",
    "    only_mutated_guides.to_csv(f'{output_folder}{gene}_TFBSoverlapping_genotyped_only_mutated.tsv', sep=\"\\t\", index=False, header=1)\n",
    "\n",
    "    #print(only_mutated_guides)\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_motifs_bed = '../../data/output/non-overlapping_includingbidirectional_all_genes_newannotation_3KB/FIMO/promoters_5UTR_motifs_mapped_q0_05.bed'\n",
    "mutations_ARF9 = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/Variant_call/ARF9_merged.tsv'\n",
    "mutations_ARF18 = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/Variant_call/ARF18_merged.tsv'\n",
    "mutations_DREB26 = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/Variant_call/DREB26_merged.tsv'\n",
    "mutations_NLP7 = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/Variant_call/NLP7_merged.tsv'\n",
    "output_folder = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/Variant_call/'\n",
    "guide_pairs = f'../../data/CRISPR_library/pacbio/demultiplexed/Data_Package_Batch_04_04_2022/Sam_Witham_EI_SW_ENQ-5142_A_01_Additional_Barcode_Analysis/References/all_guide_pairs.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files\n",
    "mutations_ARF9_df,mapped_motifs_df,guide_pairs_df =  read_in_files(mutations_ARF9, mapped_motifs_bed, guide_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutations_df = find_overlapping_TFBSs(mutations_ARF9_df,mapped_motifs_df,output_folder,'ARF9')\n",
    "mutations_df = chunkify(mutations_ARF9_df,mapped_motifs_df,output_folder,'ARF9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations_df_genotyped = genotype_plant_lines(mutations_df,output_folder,'ARF9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorise_guide_pairs(mutations_df_genotyped,guide_pairs_df,output_folder,'ARF9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2879a2396fb26a2192d4c546fae66e67014459dff7902256699e5c7228156d44"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pybedtools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2879a2396fb26a2192d4c546fae66e67014459dff7902256699e5c7228156d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
