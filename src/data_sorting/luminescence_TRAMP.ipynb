{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<b> Sort the protoplast luminescence data from the xlsx output from the Glariostar platereader. \n",
    "Use 2 input excels at a time (one firefly, one nanoluc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "def xlsx_2_csv(xlsx):  \n",
    "    \"\"\" Function to read and convert xlsx file to csv file. Also return the data (name of the folder the xlsx is in)\"\"\"\n",
    "    \n",
    "    # Read in the xlsx file, second sheet\n",
    "    file = pd.read_excel(xlsx, 'Table End point', index_col=None) \n",
    "    \n",
    "    filename = os.path.basename(xlsx)\n",
    "    removed_extension = os.path.splitext(filename)[0]\n",
    "    path = Path(xlsx).parent #find parent directory to the one the xlsx fields are in\n",
    "    date = Path(xlsx)\n",
    "    \n",
    "    file.to_csv(f'{path}/{removed_extension}.csv', encoding='utf-8', index=False)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "def combine_csvs(input_luc,layout_csv,picogreen_csv,date,output_file_means,output_file_raw):\n",
    "    \"\"\"Function to combine two csv files containingg luminescence data, and label values using layout csv file (plate layout)\"\"\"\n",
    "    #read in files\n",
    "    luc = pd.read_csv(input_luc, header=0)\n",
    "    picogreen = pd.read_csv(picogreen_csv, header=0)\n",
    "    layout_df = pd.read_csv(layout_csv, header=0)\n",
    "    #make new df with correct column names\n",
    "    combined = luc[['Well', 'Content','Blank corrected based on Raw Data (No filter)']].copy()\n",
    "    pico = picogreen[['Well', 'Content','Blank']].copy()\n",
    "    #add well row and column columns\n",
    "    combined['well_row'] = combined.Well.str[:1]\n",
    "    combined['well_col'] = combined.Well.str[-2:]    \n",
    "    combined.rename(columns = {'Content':'content', 'Blank corrected based on Raw Data (No filter)':'luminescence'}, inplace = True)\n",
    "    pico['well_row'] = pico.Well.str[:1]\n",
    "    pico['well_col'] = pico.Well.str[-2:]    \n",
    "    pico.rename(columns = {'Content':'content', 'Blank':'luminescence'}, inplace = True)\n",
    "    #prepend layout well col with a 0\n",
    "    layout = layout_df.copy()\n",
    "    layout['well_col'] = layout_df['well_col'].astype(str).str.zfill(width=2)\n",
    "    \n",
    "    # #mask any values less than 400 (turn into NaNs)  \n",
    "    combined['luminescence'] = combined.luminescence.mask(combined.luminescence < 400)\n",
    "    # #merge picogreen with combine\n",
    "    #change df content data type to string\n",
    "    combined.content = combined.content.astype(str)\n",
    "    #combined_pico = pd.merge(combined, pico, how='outer')\n",
    "    #merge DNA_only pico green with layout\n",
    "    merged_pico = pd.merge(pico, layout, how='outer')\n",
    "  \n",
    "    #combined_named = pd.concat([combined, layout])\n",
    "    \n",
    "    #merge with layout\n",
    "\n",
    "\n",
    "    combined_named = pd.merge(combined, layout, how='outer')\n",
    "    #combined_pico = pd.concat([combined_named, pico])\n",
    "\n",
    "    # #convert well_col column data type to string so it is excluded from the next bit\n",
    "    combined_named.well_col = combined_named.well_col.astype(np.str)\n",
    "    merged_pico.well_col = merged_pico.well_col.astype(np.str)\n",
    "    # #add new column, nluc/fluc\n",
    "    # combined_named['nluc/fluc'] = combined_named.nluc_luminescence / combined_named.fluc_luminescence\n",
    "    #remove NaNs\n",
    "    combined_named_no_null = combined_named[pd.notnull(combined_named['luminescence'])]\n",
    "    merged_pico_no_null = merged_pico[pd.notnull(merged_pico['luminescence'])]\n",
    "    #make new df with mean pico\n",
    "    mean_pico = merged_pico_no_null[['name','luminescence']].groupby(['name',]).mean().reset_index()\n",
    "    #rename name column\n",
    "    mean_pico.name = mean_pico.name.str[:-9]\n",
    "    #changed column name\n",
    "    mean_pico.rename(columns = {'luminescence':'mean_luminescence_pico'}, inplace = True)\n",
    "    #merge mean pico with combined_named_no_null\n",
    "    merged_for_correction = pd.merge(combined_named_no_null,mean_pico, how='left', on='name')\n",
    "    #divide luminescence by pico DNA luminescence to correct for volume of DNA\n",
    "    combined_named_no_null_corrected = merged_for_correction.copy()\n",
    "    combined_named_no_null_corrected['corrected_luminescence'] = merged_for_correction.luminescence/merged_for_correction['mean_luminescence_pico']\n",
    "\n",
    "    #put DNA_only names on same\n",
    "    # #add date to the data\n",
    "    combined_named_no_null_date = combined_named_no_null_corrected.copy()\n",
    "    combined_named_no_null_date['date'] = date\n",
    "    #make csv of raw data\n",
    "    combined_named_no_null_date.to_csv(output_file_raw, encoding='utf-8', index=False)\n",
    "    #make new df with mean luminescence, mean picogreen\n",
    "    mean = combined_named_no_null_corrected[['name','corrected_luminescence']].groupby(['name',]).mean().reset_index()\n",
    "\n",
    "\n",
    "    # ######mean = combined_named_no_null[['name', 'nluc/fluc']].groupby('name').mean().reset_index()\n",
    "    mean.rename(columns = {'luminescence':'mean_luminescence'}, inplace = True)\n",
    "    #merge rows\n",
    "    #name = pd.DataFrame({'name':mean['name'].iloc[::2].values, 'dna_name':mean['name'].iloc[1::2].values})\n",
    "    #lum = pd.DataFrame({'mean_luminescence':mean['mean_luminescence'].iloc[::2].values, 'dna_mean_luminescence':mean['mean_luminescence'].iloc[1::2].values})\n",
    "    #merge on index\n",
    "    #new_mean = pd.merge(name,lum, left_index=True, right_index=True)\n",
    "    #new_mean.groupby('name')['dna_name'].apply(','.join).reset_index()\n",
    "    \n",
    "    #divide mean_luminescence of TRAMP assay by the picogreen mean_luminescence to control for concentration of DNA\n",
    "    #new_mean['mean_corrected_luminescence'] = new_mean['mean_luminescence']/new_mean['dna_mean_luminescence']\n",
    "\n",
    "    # #add standard error\n",
    "    #first remove DNA_only rows\n",
    "    #combined_named_no_null_no_DNA = combined_named_no_null[~combined_named_no_null['name'].str.contains('DNA_only')]\n",
    "    standard_error = combined_named_no_null_corrected[['name', 'corrected_luminescence']].groupby(['name']).sem().reset_index()\n",
    "    # #####standard_error = combined_named_no_null[['name','nluc/fluc']].groupby('name').sem().reset_index()\n",
    "    standard_error.rename(columns = {'corrected_luminescence':'standard_error'}, inplace=True)\n",
    "    mean_samples = pd.merge(mean, standard_error, on=['name'])\n",
    "    # #####mean_samples = pd.merge(mean, standard_error, on='name')\n",
    "    # #add date of experiment\n",
    "    mean_samples['date'] = date\n",
    "    # #create output file\n",
    "    mean_samples.to_csv(output_file_means, encoding='utf-8', index=False)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#find all xlsx files recursively in the 'to_be_sorted' folder\n",
    "xlsx_filenames = glob.glob('../../data/luminescence/TRAMP/**/*.xlsx', recursive=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "#run the xlsx_2_csv function across all xlsx file in to_be_sorted folder\n",
    "list(map(xlsx_2_csv,xlsx_filenames))               \n",
    "                 "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "XLRDError",
     "evalue": "No sheet named <'Table End point'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/xlrd/book.py\u001b[0m in \u001b[0;36msheet_by_name\u001b[0;34m(self, sheet_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0msheetx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sheet_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'Table End point' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mXLRDError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-11beb3d2065d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#run the xlsx_2_csv function across all xlsx file in to_be_sorted folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlsx_2_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxlsx_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-c23f3e3f5785>\u001b[0m in \u001b[0;36mxlsx_2_csv\u001b[0;34m(xlsx)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Read in the xlsx file, second sheet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlsx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Table End point'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlsx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mconvert_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0mconvert_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmangle_dupe_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m         )\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# assume an integer if not a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masheetname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mget_sheet_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sheet_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PromoterArchitecturePipeline/lib/python3.7/site-packages/xlrd/book.py\u001b[0m in \u001b[0;36msheet_by_name\u001b[0;34m(self, sheet_name)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0msheetx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sheet_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mXLRDError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No sheet named <%r>'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msheetx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXLRDError\u001b[0m: No sheet named <'Table End point'>"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "use os.scandir when scanning a directory, this is the fastest way according to Matt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# input_fluc = '../../data/luminescence/to_be_sorted/24.11.19/nitrate_free_phytogel_fluc.csv'\n",
    "# input_nluc = '../../data/luminescence/to_be_sorted/24.11.19/nitrate_free_phytogel_nluc.csv'\n",
    "# layout = '../../data/luminescence/to_be_sorted/24.11.19/layout.csv'\n",
    "# output = '../../data/luminescence/to_be_sorted/24.11.19/output_means.csv'\n",
    "# output_raw = '../../data/luminescence/to_be_sorted/24.11.19/output_raw.csv'\n",
    "# date = '24.11.19'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "date = '22.6.21'\n",
    "input_luc = f'../../data/luminescence/TRAMP/{date}/22.6.21_TRAMP_ARF18_usethis.csv'\n",
    "picogreen_csv = f'../../data/luminescence/TRAMP/{date}/22.6.21_picogreen_ARF18probes_repeat3_usethis.csv'\n",
    "layout = f'../../data/luminescence/TRAMP/{date}/layout.csv'\n",
    "output = f'../../data/luminescence/TRAMP/{date}/output_means.csv'\n",
    "output_raw = f'../../data/luminescence/TRAMP/{date}/output_raw.csv'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "combine_csvs(input_luc,layout,picogreen_csv,date,output, output_raw)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PromoterArchitecturePipeline] *",
   "language": "python",
   "name": "conda-env-PromoterArchitecturePipeline-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}